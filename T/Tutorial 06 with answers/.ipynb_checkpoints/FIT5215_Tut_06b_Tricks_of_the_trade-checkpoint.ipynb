{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT5215: Deep Learning (2021)</span>\n",
    "***\n",
    "*CE/Lecturer:*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Head TA:*  **Dr Van Nguyen** | van.nguyen1@monash.edu <br/>\n",
    "*Tutor:* **Mr Anh Bui** \\[tuananh.bui@monash.edu\\] | **Mr Tuan Nguyen**  \\[tuan.ng@monash.edu \\] | **Dr Binh Nguyen** \\[binh.nguyen1@monash.edu\\] | **Dr Mahmoud Mohammad** \\[mahmoud.hossam@monash.edu\\]\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tutorial 06b (Additional Reading): Tricks of The Trade for TF 1.x</span><span style=\"color:red;  font-size: 18px\">** (reference)</span> #\n",
    "\n",
    "**The purpose of this tutorial is to demonstrate the tricks of the trade in deep learning. The following topics are presented:**\n",
    "1. How to apply weight initialization, gradient clipping, batch normalization, and dropout to regularize deep networks and stabilize a training process.\n",
    "2. How to reuse a pre-trained model and further do fine-tuning.\n",
    "3. How to reuse a part of a pre-trained model to build up a new model.\n",
    "\n",
    "***Acknowledgement***: *this tutorial was developed based on the Chapter 11 materials from the book `Hands-on Machine Learning with Scikit-learn and Tensorflow`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">I. Preparation</span> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we do some preparation in advance by importing necessary libraries and modules as well as doing some setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "# Common imports\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "\n",
    "# To make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">II. Weight Initializier, User-defined Activation, and Batch Normalization</span> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">II.1. Non-saturating Leaky ReLU activation function</span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `Leaky ReLU` activation function, which is defined as $leaky\\_relu(z)=\\max\\{\\alpha z, z\\}$ where $\\alpha$ is a parameter. Unlike `sigmoid`, and `tanh`, `Leaky ReLU` is non-saturating. We now declare this function and visualize this activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">II.2. Data processing</span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `tf.keras.datasets.mnist` to load the `MNIST` dataset since `tf.examples.tutorials.mnist` is deprecated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code to randomly generate mini-batches. We first permuate the training set $X, y$ and split them into mini-batches. Note that the keyword `yield` helps us create a generator of mini-batches.\n",
    "- Generators are iterators, but you can only iterate over them once. It’s because they do not store all the values in memory, they generate the values on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">II.3. Xavier and He initialization</span> ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training a deep learning model, the `initialization of weight matrices` is quite important. We need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. We don’t want the signal to die out, nor do we want it to explode and saturate.\n",
    "\n",
    "There are two well-known initialization strategies, namely `Xavier` and `He` initializers. The following tables summarize the technical details of these two initializers. <br/><br/>\n",
    "\n",
    "<img src=\"./images/initializers.png\" align=\"center\" width=600/>\n",
    "\n",
    "It is very convenient to apply `Xavier` and `He` initializers in `TensorFlow`\n",
    "- **He initializer**: `he_init = tf.contrib.layers.variance_scaling_initializer()` and  `hidden1 = fully_connected(X, n_hidden1, weights_initializer=he_init, scope=\"h1\")`\n",
    "- **Xavier initializer**: `xavier_init = tf.contrib.layers.xavier_initializer()` and  `hidden1 = fully_connected(X, n_hidden1, weights_initializer=xavier_init, scope=\"h1\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">II.4. Batch normalization</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First introduced by Ioffe and Szgedy in their 2015 paper, *Batch Normalization: Accelerating Deep\n",
    "Network Training by Reducing Internal Covariate Shift*, batch normalization layers (or BN for\n",
    "short), as the name suggests, are used to normalize the activations of a given input volume before\n",
    "passing it into the next layer in the network.\n",
    "\n",
    "The technique consists of adding an operation in the model just after the activation\n",
    "function of each layer, simply zero-centering and normalizing the inputs, then scaling\n",
    "and shifting the result using two new parameters per layer (one for scaling, the other\n",
    "for shifting). In other words, this operation lets the model learn the optimal scale and\n",
    "mean of the inputs for each layer.\n",
    "- The order of operations is `INPUT => CONV => ACT => BN`\n",
    "\n",
    "Batch normalization has been shown to be extremely effective at reducing the number of\n",
    "epochs it takes to train a neural network. Batch normalization also has the added benefit of helping\n",
    "“stabilize” training, allowing for a larger variety of learning rates and regularization strengths. Using\n",
    "batch normalization doesn’t alleviate the need to tune these parameters of course, but it will make\n",
    "your life easier by making the learning rate and regularization less volatile and more straightforward\n",
    "to tune.\n",
    "\n",
    "**Training phase**\n",
    "- Let $x$ be our mini-batch after activation. We then compute the normalized $\\hat{x}$ via\n",
    "the following equation:\n",
    "  - $\\hat{x}=\\frac{x-\\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$ where $\\epsilon$ is a small positive value such as $1e-7$\n",
    "  - The empirical mean $\\mu_B=\\frac{1}{m}\\sum_{i=1}^{m}x_i$ where $m$ is the batch size\n",
    "  - The empirical variance $\\sigma_B^{2}= \\frac{1}{m}\\sum_{i=1}^{m}\\left(x_i-\\mu_B\\right)^2$\n",
    "- We scale the normalized $\\hat{x}$\n",
    "  - $z_i=\\gamma \\hat{x}_i + \\beta$ where $\\gamma, \\beta>0$ are two learning parameters\n",
    " \n",
    "**Testing phase**\n",
    "-  At testing time, we replace the mini-batch $\\mu_B$ and $\\sigma_B$ with running averages of $\\tilde{\\mu_B}$ and $\\tilde{\\sigma_B}$ computed during the training process.\n",
    "   - $\\tilde{\\mu_B}=(1-\\theta) \\tilde{\\mu_B} + \\theta \\mu_B$ and $\\tilde{\\sigma_B}^2=(1-\\theta)\\tilde{\\sigma_B}^2 + \\theta \\sigma_B^2$ where $0< \\theta <1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">II.5. Dropout</span> ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is actually a form of regularization that aims to help prevent overfitting by increasing testing accuracy, perhaps at the expense of training accuracy. For each mini-batch in our training set, dropout layers, with probability $p$,\n",
    "randomly disconnect inputs from the preceding layer to the next layer in the network architecture.\n",
    "\n",
    "The following figure visualizes this concept where we randomly disconnect with probability $p = 0.5$ the\n",
    "connections between two FC layers for a given mini-batch. Again, notice how half of the connections are severed for this mini-batch. After the forward and backward passes are computed for the minibatch, we re-connect the dropped connections, and then sample another set of connections to drop.\n",
    "\n",
    "<img src=\"./images/dropout.png\" align=\"center\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">II.6. All together</span> ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare `Leaky ReLU` using `TensorFlow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha= 0.01, name=None):\n",
    "    return tf.maximum(alpha * z, z, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#0b486b\">**Construction phase** </span> <br/>\n",
    "\\- To avoid repeating the code, we predefine `my_batch_norm_layer` and `my_dense_layer` using the `partial` function in `functools`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "batch_norm_momentum = 0.9\n",
    "learning_rate= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            activation= leaky_relu,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = my_batch_norm_layer(hidden1)\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = my_batch_norm_layer(hidden2)\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        training_op = optimizer.minimize(loss)\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#0b486b\">**Execution phase** </span> <br/>\n",
    "\\- Since we are using `tf.layers.batch_normalization()` rather than `tf.contrib.layers.batch_norm()`, we need to explicitly run the extra update operations needed by batch normalization (`sess.run([training_op, extra_update_ops],...`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train accuracy: 0.96125454 \tValidation accuracy: 0.9576\n",
      "2 Train accuracy: 0.9745273 \tValidation accuracy: 0.9666\n",
      "3 Train accuracy: 0.98145455 \tValidation accuracy: 0.9712\n",
      "4 Train accuracy: 0.98661816 \tValidation accuracy: 0.9768\n",
      "5 Train accuracy: 0.9891091 \tValidation accuracy: 0.9758\n",
      "6 Train accuracy: 0.9912 \tValidation accuracy: 0.9786\n",
      "7 Train accuracy: 0.99243635 \tValidation accuracy: 0.9802\n",
      "8 Train accuracy: 0.9944 \tValidation accuracy: 0.9774\n",
      "9 Train accuracy: 0.99523634 \tValidation accuracy: 0.9792\n",
      "10 Train accuracy: 0.99645454 \tValidation accuracy: 0.9806\n",
      "11 Train accuracy: 0.9969818 \tValidation accuracy: 0.9812\n",
      "12 Train accuracy: 0.9971273 \tValidation accuracy: 0.979\n",
      "13 Train accuracy: 0.9977818 \tValidation accuracy: 0.981\n",
      "14 Train accuracy: 0.9979454 \tValidation accuracy: 0.98\n",
      "15 Train accuracy: 0.9981091 \tValidation accuracy: 0.9816\n",
      "16 Train accuracy: 0.9985091 \tValidation accuracy: 0.983\n",
      "17 Train accuracy: 0.9986182 \tValidation accuracy: 0.9816\n",
      "18 Train accuracy: 0.99889094 \tValidation accuracy: 0.9802\n",
      "19 Train accuracy: 0.9989091 \tValidation accuracy: 0.9814\n",
      "20 Train accuracy: 0.99907273 \tValidation accuracy: 0.9826\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs=20\n",
    "batch_size=32\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run([training_op],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        accuracy_train= accuracy.eval(feed_dict={X:X_train, y:y_train})\n",
    "        print(epoch+1, \"Train accuracy:\", accuracy_train, \"\\tValidation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 1</span>:** Rewrite the above code to change the order of batch normalization layer as `INPUT => CONV => BN => ACT`. This is the order proposed in the original paper of batch normalization. Compare the experimental results of the two orders of batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 2</span>:** Rewrite the above code to apply the dropout to the `hidden1` and `hidden2` layers with the dropout rate $r=0.2$. Note that you can use the `tf.layers.dropout` as shown in [this link](https://www.tensorflow.org/api_docs/python/tf/layers/dropout). Compare to the case without using the dropout technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">III. Gradient Clipping</span> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular technique known as `gradient clipping` to lessen the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold (this is mostly useful for recurrent neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#0b486b\">**Construction phase** </span> <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#0b486b\">**Applying gradient clipping** </span> <br/>\n",
    "\\- We clip the gradients with respect to the variables to a predefined interval and then apply the capped gradients when updating these variables (i.e., weight matrices and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate= 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#0b486b\">**Execution phase** </span> <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train accuracy: 0.29654545 \tValidation accuracy: 0.2876\n",
      "2 Train accuracy: 0.78532726 \tValidation accuracy: 0.7942\n",
      "3 Train accuracy: 0.87398183 \tValidation accuracy: 0.8794\n",
      "4 Train accuracy: 0.8998 \tValidation accuracy: 0.906\n",
      "5 Train accuracy: 0.91007274 \tValidation accuracy: 0.9162\n",
      "6 Train accuracy: 0.9156182 \tValidation accuracy: 0.9214\n",
      "7 Train accuracy: 0.9244182 \tValidation accuracy: 0.929\n",
      "8 Train accuracy: 0.9322 \tValidation accuracy: 0.9356\n",
      "9 Train accuracy: 0.9336182 \tValidation accuracy: 0.9382\n",
      "10 Train accuracy: 0.939 \tValidation accuracy: 0.9416\n",
      "11 Train accuracy: 0.9438364 \tValidation accuracy: 0.9456\n",
      "12 Train accuracy: 0.9468727 \tValidation accuracy: 0.947\n",
      "13 Train accuracy: 0.94534546 \tValidation accuracy: 0.9476\n",
      "14 Train accuracy: 0.95207274 \tValidation accuracy: 0.953\n",
      "15 Train accuracy: 0.9558182 \tValidation accuracy: 0.9564\n",
      "16 Train accuracy: 0.9562 \tValidation accuracy: 0.9566\n",
      "17 Train accuracy: 0.9590182 \tValidation accuracy: 0.9576\n",
      "18 Train accuracy: 0.9605455 \tValidation accuracy: 0.9588\n",
      "19 Train accuracy: 0.96314543 \tValidation accuracy: 0.9624\n",
      "20 Train accuracy: 0.9607818 \tValidation accuracy: 0.9612\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        accuracy_train= accuracy.eval(feed_dict={X:X_train, y:y_train})\n",
    "        print(epoch+1, \"Train accuracy:\", accuracy_train, \"\\tValidation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">III. Reusing Pretrained Layers</span> ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">III.1. Reusing a TensorFlow Model </span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load the graph's structure. The `import_meta_graph()` function does just that, loading the graph's operations into the default graph, and returning a `Saver` that we can then use to restore the model's state. Note that by default, a `Saver` saves the structure of the graph into a `.meta` file, so that's the file we should load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    file_writer = tf.summary.FileWriter(logdir='logs/my-model', session=sess) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to get a handle on all the operations we will need for training. If we don't know the graph's structure, we can list all the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "gradients/Shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/loss/loss_grad/Reshape\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "eval/in_top_k/InTopKV2/k\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know which operations you need, we can get a handle on them using the graph's `get_operation_by_name()` or `get_tensor_by_name()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are the author of the original model, we could make things easier for people who will reuse your model by giving operations very clear names and documenting them. Another approach is to create a collection containing all the important operations that people will want to get a handle on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way people who reuse our model will be able to simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start a session, restore the model's state and continue training on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "1 Train accuracy: 0.9667636 \tValidation accuracy: 0.9632\n",
      "2 Train accuracy: 0.96616364 \tValidation accuracy: 0.9646\n",
      "3 Train accuracy: 0.9689091 \tValidation accuracy: 0.9648\n",
      "4 Train accuracy: 0.9703636 \tValidation accuracy: 0.9642\n",
      "5 Train accuracy: 0.97041816 \tValidation accuracy: 0.966\n",
      "6 Train accuracy: 0.972 \tValidation accuracy: 0.9672\n",
      "7 Train accuracy: 0.9737818 \tValidation accuracy: 0.9682\n",
      "8 Train accuracy: 0.9738182 \tValidation accuracy: 0.9664\n",
      "9 Train accuracy: 0.9723273 \tValidation accuracy: 0.966\n",
      "10 Train accuracy: 0.97581816 \tValidation accuracy: 0.9712\n",
      "11 Train accuracy: 0.9753091 \tValidation accuracy: 0.9674\n",
      "12 Train accuracy: 0.9774909 \tValidation accuracy: 0.9704\n",
      "13 Train accuracy: 0.9790364 \tValidation accuracy: 0.971\n",
      "14 Train accuracy: 0.9799273 \tValidation accuracy: 0.9706\n",
      "15 Train accuracy: 0.9804909 \tValidation accuracy: 0.9696\n",
      "16 Train accuracy: 0.9794 \tValidation accuracy: 0.9702\n",
      "17 Train accuracy: 0.97974545 \tValidation accuracy: 0.9712\n",
      "18 Train accuracy: 0.9823091 \tValidation accuracy: 0.9688\n",
      "19 Train accuracy: 0.98225456 \tValidation accuracy: 0.9726\n",
      "20 Train accuracy: 0.9834545 \tValidation accuracy: 0.9716\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        accuracy_train= accuracy.eval(feed_dict={X:X_train, y:y_train})\n",
    "        print(epoch+1, \"Train accuracy:\", accuracy_train, \"\\tValidation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if we have access to the Python code that built the original graph, we can use it instead of `import_meta_graph()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And continue training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "1 Train accuracy: 0.96516365 \tValidation accuracy: 0.9642\n",
      "2 Train accuracy: 0.96785456 \tValidation accuracy: 0.9628\n",
      "3 Train accuracy: 0.96681815 \tValidation accuracy: 0.9654\n",
      "4 Train accuracy: 0.9696364 \tValidation accuracy: 0.9652\n",
      "5 Train accuracy: 0.9694 \tValidation accuracy: 0.9642\n",
      "6 Train accuracy: 0.96721816 \tValidation accuracy: 0.9648\n",
      "7 Train accuracy: 0.97318184 \tValidation accuracy: 0.969\n",
      "8 Train accuracy: 0.9746182 \tValidation accuracy: 0.9686\n",
      "9 Train accuracy: 0.9746364 \tValidation accuracy: 0.9684\n",
      "10 Train accuracy: 0.9762 \tValidation accuracy: 0.9684\n",
      "11 Train accuracy: 0.9770909 \tValidation accuracy: 0.9704\n",
      "12 Train accuracy: 0.9771091 \tValidation accuracy: 0.9714\n",
      "13 Train accuracy: 0.9748727 \tValidation accuracy: 0.9672\n",
      "14 Train accuracy: 0.97816366 \tValidation accuracy: 0.9702\n",
      "15 Train accuracy: 0.98054546 \tValidation accuracy: 0.9712\n",
      "16 Train accuracy: 0.98 \tValidation accuracy: 0.9724\n",
      "17 Train accuracy: 0.98234546 \tValidation accuracy: 0.972\n",
      "18 Train accuracy: 0.9824182 \tValidation accuracy: 0.971\n",
      "19 Train accuracy: 0.9833818 \tValidation accuracy: 0.9714\n",
      "20 Train accuracy: 0.9815636 \tValidation accuracy: 0.9714\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        accuracy_train= accuracy.eval(feed_dict={X:X_train, y:y_train})\n",
    "        print(epoch+1, \"Train accuracy:\", accuracy_train, \"\\tValidation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">III.2. Reusing a Part of a Pretrained TensorFlow Model </span> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, we only want to reuse a part of a pretrained model and further build up additional layers over previous pretrained model. \n",
    "\n",
    "In general, we want to reuse only the lower layers. If we are using `import_meta_graph()` it will load the whole graph, but we can simply ignore the parts we do not need.\n",
    "\n",
    "In this example, we add a new 4th hidden layer on top of the pretrained 3rd layer (ignoring the old 4th hidden layer). We also build a new output layer, the loss for this new output, and a new optimizer to minimize it. We also need another saver to save the whole graph (containing both the entire old graph plus the new operations), and an initialization operation to initialize all the new variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20  # new layer\n",
    "n_outputs = 10  # new layer\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden3/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can train this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "1 Train accuracy: 0.90352726 \tValidation accuracy: 0.9124\n",
      "2 Train accuracy: 0.92896366 \tValidation accuracy: 0.938\n",
      "3 Train accuracy: 0.9373818 \tValidation accuracy: 0.946\n",
      "4 Train accuracy: 0.9445636 \tValidation accuracy: 0.9498\n",
      "5 Train accuracy: 0.9488 \tValidation accuracy: 0.9528\n",
      "6 Train accuracy: 0.9497455 \tValidation accuracy: 0.9526\n",
      "7 Train accuracy: 0.9553273 \tValidation accuracy: 0.9566\n",
      "8 Train accuracy: 0.95870906 \tValidation accuracy: 0.96\n",
      "9 Train accuracy: 0.9604727 \tValidation accuracy: 0.9616\n",
      "10 Train accuracy: 0.9621636 \tValidation accuracy: 0.961\n",
      "11 Train accuracy: 0.96483636 \tValidation accuracy: 0.9632\n",
      "12 Train accuracy: 0.96494544 \tValidation accuracy: 0.9628\n",
      "13 Train accuracy: 0.964 \tValidation accuracy: 0.965\n",
      "14 Train accuracy: 0.96805453 \tValidation accuracy: 0.9654\n",
      "15 Train accuracy: 0.96952724 \tValidation accuracy: 0.9666\n",
      "16 Train accuracy: 0.97032726 \tValidation accuracy: 0.967\n",
      "17 Train accuracy: 0.97185457 \tValidation accuracy: 0.9678\n",
      "18 Train accuracy: 0.97187275 \tValidation accuracy: 0.9678\n",
      "19 Train accuracy: 0.9734182 \tValidation accuracy: 0.9684\n",
      "20 Train accuracy: 0.9721091 \tValidation accuracy: 0.9676\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        accuracy_train= accuracy.eval(feed_dict={X:X_train, y:y_train})\n",
    "        print(epoch+1, \"Train accuracy:\", accuracy_train, \"\\tValidation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.5",
   "language": "python",
   "name": "tf2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
